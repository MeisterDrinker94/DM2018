\documentclass{article}
\usepackage[utf8]{inputenc}

\title{DiSH - Detecting Subspace cluster Hierarchies \\ Data Mining- Assignment 1}
\author{Konrad von Kirchbach, Donatella Novakovic, \\ Wolfgang Ost, Jakob Weber}
\date{November 2018}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subfigure}
\usepackage{amssymb}

\begin{document}

\maketitle

\section{Introduction}
DiSH (Detecting Subspace cluster Hierarchies) belongs to the group of subspace clustering algorithms and is an advanced version of HiSC (Finding Hierarchies of Subspace Clusters) considering the discovery of more complex hierarchies. Both, DiSH and HiSC are subspace clustering methods based on OPTICS (Ordering points to identify the clustering structure). DiSH is intended to eliminate the limitations of clustering algorithms given by the approach of generating non-overlapping clusters. More precisely, these algorithms assign points uniquely to one cluster or noise. The aim of DiSH is to enable improvement in following properties over the state-of-the-art subspace clustering approaches: 
\begin{itemize}
    \item Uncovering of complex hierarchies of nested subspace clusters
    \item Detection of clusters in subspaces of different dimensionality
    \item Ability to detect clusters of different size, shape and density
\end{itemize}

This elaboration deals with the implementation of the algorithm DiSH according to explanations and findings of Achtert et al. and the further comparison of results with the algorithms implemented in Environment for DeveLoping KDD-Applications Supported by Index-Structures (ELKI). The DiSH algorithm is first evaluated on a generated synthetic data set, which shall point out the properties and advantages of this method. Secondly, as the Achtert et al. suggest, the Wages\footnote{htpp://lib.stat.cmu.edu} data set is used. Results of both data sets are evaluated using the implemented DiSH algorithm and compared to the DiSH algorithm implemented in ELKI.    

\section{Implementation}
This section discusses the approach of implementing the algorithm DiSH and addresses occurring difficulties and uncertainties during this task. The implementation process is fully based on the paper \emph{Detection and Visualization of Subspace Cluster Hierarchies}, published by E. Alchert et al., which describes the the structure and procedure of DiSH. Following pseudo codes are adapted from the paper:\par

\begin{algorithm}
\caption{DiSH Algorithm}
\begin{algorithmic}[1]
\Procedure{DiSH}{$D, \mu, \epsilon$}
    \State $co\leftarrow$ cluster order; \Comment{initially empty}
    \State $pq\leftarrow$ empty priority queue ordered by $REACHDIST_{\mu};$
    \For{$p\in D$}
        \State compute $w(p)$;
        \State $p.REACHDIST_{\mu} \leftarrow \infty$;
        \State insert $p$ into $pq$;
    \EndFor
    \While{($pq \neq 0$)}
        \State $o\leftarrow pq.next()$;
        \State $\mu$-nearest neighbor of o w.r.t. $SDIST$;
        \For{$p\in pq$}
            \State $new_sr\leftarrow max(SDIST(o,r), SDIST(o,p))$;
            \State $pq.update(p, new_sr)$;
        \EndFor%\label{dishfor}
        \State append $o$ to $co$;
    \EndWhile%\label{dishwhile}
    \State\textbf{return} $co$;
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Extraction of Clusters from Cluster Order}
\begin{algorithmic}[1]
\Procedure{method extractCluster}{ClusterOrder $co$}
    \State $cl \leftarrow$ empty list; \Comment{cluster list}
    \For{$o \in co$}
        \State $p\leftarrow o.predecessor$;
        \If{($\nexists c\in cl$ with $w(c)=w(o,p)\wedge dist_{w(o,p)}(o, c.center)\leq 2*\epsilon$)}
            \State create a new $c$;
            \State add $c$ to $cl$;
        \EndIf
        \State add $o$ to $c$;
    \EndFor
    \State\textbf{return} $cl$;
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{method buildHierarchy($cl$)}
\begin{algorithmic}
\Procedure{method buildHierarchy}{$cl$}
    \State $d\leftarrow$ dimensionality of objects in $D$;
    \For{$c_{i} \in cl$}
        \For{$c_{j} \in cl$}
            \If{($\lambda_{c_{j}} > \lambda_{c_{i}}$)}
                \State $d\leftarrow dist_{w(c_{i},c_{j})}(c_{i}.center,c_{j}.center)$;
                \If{($\lambda_{c_{j}}=d\vee(\emph{d} \leq 2*\epsilon \wedge \nexists c \in cl : c \in c_{i}.parents \wedge \lambda_{c} < \lambda_{c_{j}}))$}
                    \State add $c_{i}$ as child to $c_{j}$
                \EndIf
            \EndIf
        \EndFor
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}

The main idea of DiSH is the definition of the subspace distance $SDIST$. So, small values are assigned if two points are in a common low-dimensional subspace cluster, whereas high values are an indicator of two points in a common high-dimensional subspace cluster or in no subspace cluster at all. Consequently, subspace clusters with a small subspace distance are embedded within clusters with a higher subspace distances.\par

The subspace dimensionality for each point $o$ in the data set $D$ is computed, assuming that the optimal subspace has the highest dimensionality or a higher number of points in the neighborhood. This step is accomplished by analyzing the variance and considering only dimensions of low variance.  In other words, attributes $a$ become a candidate for a subspace if an attribute-wise $\epsilon$ query yields at least $\mu$ objects. Subsequently, these attributes must be combined in an optimal way. Therefore, we used the heuristic best-first search approach, presented in the paper, which scales linearly in the number of dimensions. Using this approach we can determine the subspace preference vectors $w(o)$. Only attributes with predicate 1 remain relevant, since these lead to the clusters. In the next step a similarity measure is defined, assigning the corresponding distance. In order to distinguish between points which are associated to the same k-dimensional subspace cluster and points associated to different k-dimensional subspace clusters, but intersect, it must be checked whether preference vector of two points are equal or if one preference vector is included in the other one. If the distance between the points in the subspace spanned by w(p,q) exceeds $2*\epsilon$, we can assume that the point belongs to parallel or intersected clusters. Similar to OPTICS, also here the distance within a subspace cluster is considered, such that the subspace clusters can exhibit arbitrary sizes shapes and densities. A factor $\mu$ is introduced and represents the minimum number of points in a cluster to achieve robustness against noise points. A so-called cluster order is extracted which constructs an order of all points using the subspace reachability distance $REACHDIST_\mu$, where $r$ is supposed to be the $\mu$-nearest neighbor w.r.t the subspace distance of $p$. Due to the data structure, for this step a naive approach running trough every single data point was our best alternative.\par  

%Heuristic Approach Pseudo Code?

However, several ambiguities arose during the implementation of DiSH. Due to some missing details and explanations in the paper, the task of implementing DiSH became rather complex. Terms, such as $\mu - nearest neighbor of o w.r.t. SDIST$ are missing some clarification, since it can be understood differently. Either $\mu -nearest neighbor$ in the computed subspace or $\mu -nearest neighbor$ across all given dimensions. Furthermore, we also noticed some inconsistencies and confusions given the naming of variables used in the pseudo code. Looking at the method \emph{buildHierachy}, there exist two variables $d$ used in two different contexts, but are not explicitly distinguished. Probably, a clearer distinction between variable-names facilitates the understanding of the pseudo code.    

\section{Evaluation and Comparison of Results with ELKI}

In order to evaluate the implemented DiSH algorithm, we generated a synthetic data set which intends to illustrate the algorithms properties. Since the paper states that none of the existing algorithms succeeded in detecting hierarchical relationships among the subspace clusters, we created the data set in such a way that one 1D cluster is embedded within two 2D clusters and tested the performance in clustering. The parameters were determined as follows: $\epsilon = 0.01$, $\mu = 50$. The same parameters were used in both implementations: executing the implemented DiSH algorithm and executing the algorithm DiSH implemented in ELKI. \emph{Figure (a), (b)} show a suitable result. Both algorithms provide a cluster hierarchy of four clusters. Moreover, \emph{Figure (b)} shows the ability of the implemented DiSH algorithm to detect a nested 1D cluster. 


\begin{figure}[h!]
\centering
    \subfigure[ELKI DiSH on Synthetic data set]{\includegraphics[width = 0.49\textwidth]{elki_synth.png}}
    \subfigure[implemented DiSH on Synthetic data set]{\includegraphics[width=0.49\textwidth]{impl_synthetic.png}}
\end{figure}


We also applied the implemented DiSH algorithm to the Wages data set, using the same parameter and information stated in the paper. Thus, we used the parameter values $\epsilon = 0.001$ and $\mu = 9$. Also we used only the in the paper stated dimensions:\emph{years of education},\emph{wage}, \emph{age} and \emph{years of work experience}, for clustering. Following figures illustrate the clustering results using DiSH in ELKI and the implemented version. Both algorithms show the same cluster hierarchy of 13 clusters. 

\begin{figure}[h!]
\centering
    \subfigure[ELKI DiSH on Wages in 2D]{\includegraphics[width = 0.49\textwidth]{elki_wages_03.png}}
    \subfigure[implemented DiSH on Wages in 2D]{\includegraphics[width= 0.49\textwidth]{impl_wages_2d.png}}
    \subfigure[ELKI DiSH on Wages, in 3D]{\includegraphics[width=0.49\textwidth]{elki_wage.png}}
    \subfigure[implemented DiSH on Wages in 3D]{\includegraphics[width= 0.49\textwidth]{impl_guteview.png}}
    \subfigure[Cluster Hierarchy Wages]{\includegraphics[width=0.30\textwidth]{elki_cluster_hierarchy_wages.PNG}}
\end{figure}


\nocite{*}
\bibliographystyle{plain}
\bibliography{references}

\end{document}
